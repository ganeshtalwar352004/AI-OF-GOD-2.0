{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61883,"databundleVersionId":6696514,"sourceType":"competition"}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Necessary Imports and Seed","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd \nimport scipy\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import WeightedRandomSampler, Dataset, DataLoader, random_split\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport timm","metadata":{"execution":{"iopub.status.busy":"2023-12-05T06:59:12.368781Z","iopub.execute_input":"2023-12-05T06:59:12.369095Z","iopub.status.idle":"2023-12-05T06:59:19.641048Z","shell.execute_reply.started":"2023-12-05T06:59:12.369067Z","shell.execute_reply":"2023-12-05T06:59:19.640268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fixing the random seed to reproduce results\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)","metadata":{"papermill":{"duration":0.011344,"end_time":"2023-10-04T15:23:25.982535","exception":false,"start_time":"2023-10-04T15:23:25.971191","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-05T06:59:19.642760Z","iopub.execute_input":"2023-12-05T06:59:19.643058Z","iopub.status.idle":"2023-12-05T06:59:19.648534Z","shell.execute_reply.started":"2023-12-05T06:59:19.643015Z","shell.execute_reply":"2023-12-05T06:59:19.647181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(42)","metadata":{"papermill":{"duration":0.014853,"end_time":"2023-10-04T15:23:26.001426","exception":false,"start_time":"2023-10-04T15:23:25.986573","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-05T06:59:19.649634Z","iopub.execute_input":"2023-12-05T06:59:19.649894Z","iopub.status.idle":"2023-12-05T06:59:19.663604Z","shell.execute_reply.started":"2023-12-05T06:59:19.649871Z","shell.execute_reply":"2023-12-05T06:59:19.662853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Visualization and Pre-Processing","metadata":{}},{"cell_type":"code","source":"# Reading the labels\ntrain_csv = pd.read_csv('/kaggle/input/ai-of-god-v20/train.csv')\ntest_csv = pd.read_csv('/kaggle/input/ai-of-god-v20/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/ai-of-god-v20/sample_submission.csv')","metadata":{"papermill":{"duration":0.050364,"end_time":"2023-10-04T15:23:26.055943","exception":false,"start_time":"2023-10-04T15:23:26.005579","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-05T06:59:39.079742Z","iopub.execute_input":"2023-12-05T06:59:39.080604Z","iopub.status.idle":"2023-12-05T06:59:39.115899Z","shell.execute_reply.started":"2023-12-05T06:59:39.080566Z","shell.execute_reply":"2023-12-05T06:59:39.114949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv","metadata":{"papermill":{"duration":0.029293,"end_time":"2023-10-04T15:23:26.089217","exception":false,"start_time":"2023-10-04T15:23:26.059924","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-12-05T06:59:39.373817Z","iopub.execute_input":"2023-12-05T06:59:39.374119Z","iopub.status.idle":"2023-12-05T06:59:39.394625Z","shell.execute_reply.started":"2023-12-05T06:59:39.374095Z","shell.execute_reply":"2023-12-05T06:59:39.393754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_csv","metadata":{"papermill":{"duration":0.019621,"end_time":"2023-10-04T15:23:26.113251","exception":false,"start_time":"2023-10-04T15:23:26.09363","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Opening an image","metadata":{}},{"cell_type":"code","source":"Image.open('/kaggle/input/ai-of-god-v20/train/3420.jpg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying basic data augmentations to prepare the input for the model. Further this block can be used to introduce Data Augmentations such as flipping, random crop, rotation, etc. Refer to the documentation of albumentations library for more data augmentations.","metadata":{}},{"cell_type":"code","source":"transform = A.Compose(\n    [\n        A.Resize(380,380),\n        A.CenterCrop(224,224),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        ToTensorV2()\n    ]\n)","metadata":{"papermill":{"duration":0.01184,"end_time":"2023-10-04T15:23:26.161915","exception":false,"start_time":"2023-10-04T15:23:26.150075","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparing the dataset class","metadata":{}},{"cell_type":"code","source":"class TrainData(Dataset):\n    def __init__(self,transform=None):\n        self.dir = \"/kaggle/input/ai-of-god-v20/train/\"\n        self.transform = transform\n    \n    def __len__(self):\n        return len(os.listdir(self.dir))\n    \n    def __getitem__(self,idx):\n        img = cv2.imread(self.dir+str(idx+1)+'.jpg')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            img = self.transform(image=img)\n        dic = {'image': img['image'],'label':train_csv['Class'][idx]}\n        return dic\n\nclass TestData(Dataset):\n    def __init__(self,transform=None):\n        self.dir = \"/kaggle/input/ai-of-god-v20/test/\"\n        self.transform = transform\n        \n    def __len__(self):\n        return len(os.listdir(self.dir))\n    \n    def __getitem__(self,idx):\n        img = cv2.imread(self.dir+str(idx+1)+'.jpg')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            img = self.transform(image=img)\n        dic = {'image':img['image']}\n        return dic","metadata":{"papermill":{"duration":0.013232,"end_time":"2023-10-04T15:23:26.1797","exception":false,"start_time":"2023-10-04T15:23:26.166468","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating an object of the dataset class and splitting the dataset into training and validation datasets. Here we will use the 90% of the data to train the model and the other 10% to evaluate the model.","metadata":{}},{"cell_type":"code","source":"# Set the split ratio\ndataset = TrainData(transform=transform)\n\n# train_ratio = 0.9\n# validation_ratio = 0.1\n\n# # Calculate the sizes for each set\n# total_size = len(dataset)\n# train_size = int(train_ratio * total_size)\n# validation_size = total_size - train_size\n\n# # Use random_split to create training and validation datasets\n# train_data, valid_data = random_split(dataset, [train_size, validation_size])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df=pd.DataFrame()\n# filename=[]\n# labels=[]\n# for i in range (len(valid_data)):\n#     filename.append(valid_data[i]['image'])\n#     labels.append(valid_data[i]['label'])\n# # # label\n# # df['Filename']=filename\n# # df['label']=label\n# # df\n# len(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oversampling of data","metadata":{}},{"cell_type":"code","source":"labels_num = train_csv.sort_values(['Class'], ascending = False)\nlabels_num = labels_num.iloc[0,1]\nlabels_num = labels_num+1\nlabels_num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_num = [0] * labels_num\n# for i in range (len(train_data)):\n#     class_num[train_data[i]['label']]+=1 \nclass_num = [0] * labels_num\nfor i in range (len(dataset)):\n    class_num[dataset[i]['label']]+=1 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = []\nfor i in range(labels_num):\n    class_weights.append(1/class_num[i])\nclass_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_weights = [0] *len(train_data)\n# for idx in range(len(train_data)):\n#         sample_weights[idx] = class_weights[train_data[idx]['label']]\nsample_weights = [0] *len(dataset)\nfor idx in range(len(dataset)):\n        sample_weights[idx] = class_weights[dataset[idx]['label']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the model","metadata":{}},{"cell_type":"markdown","source":"With the CFG class, you can set the model you want to use, the number of labels (in our case we have 8 labels), batch size for mini batch gradient descent, number of epochs, number of workers, and the learning rate.","metadata":{}},{"cell_type":"markdown","source":" class CFG:\n    model_name = 'efficientnet_b4'\n    target_size = 8\n    batch_size = 16\n    epochs = 20\n    num_workers = 2 if torch.cuda.is_available() else 4\n    lr = ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    model_name = 'efficientnet_b4' \n    target_size = 8 \n    batch_size = 16 \n    epochs = 20 \n    num_workers = 2 if torch.cuda.is_available() else 4 \n    lr =1e-5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ","metadata":{}},{"cell_type":"markdown","source":" Here we have used ResNet18 as the sample model. You can explore other models as well to improve upon your results. Based on the CFG class we have defined the CustomNet model class.","metadata":{}},{"cell_type":"code","source":"# optimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomNet(nn.Module):\n    def __init__(self, model_name=CFG.model_name, pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(CFG.model_name, pretrained=pretrained, num_classes=CFG.target_size)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the functions for training, validation, and generating the labels on the test data respectively. The training and validation functions act on one epoch.","metadata":{}},{"cell_type":"code","source":"# class FocalLoss(nn.Module):\n#     def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n#         super(FocalLoss, self).__init__()\n#         self.alpha = alpha\n#         self.gamma = gamma\n#         self.logits = logits\n#         self.reduce = reduce\n\n#     def forward(self, inputs, targets):\n#         BCE_loss = nn.CrossEntropyLoss()(inputs, targets)\n\n#         pt = torch.exp(-BCE_loss)\n#         F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n#         if self.reduce:\n#               return torch.mean(F_loss)\n#         else:\n#               return F_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l=[1,2,3,4]\n# x=[1.1,2.2,3.3,4.4]\n# label=FocalLoss()\n# label.forward(l,x)\n# label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_fn(train_loader, model, optimizer, criterion, device):\n    model.to(device).train()\n    epoch_loss = 0\n    count = 0\n    \n    for i, data in enumerate(tqdm(train_loader,total = len(train_loader))):\n        images = data['image'].to(device)\n        label = data['label'].to(device)\n\n        output = model(images)\n#         print(output.shape,label.shape)\n#         output=nn.Softmax(dim=1)(output)\n#         output=torch.argmax(output,dim=1)\n#         print(output.shape,label.shape)\n        loss = criterion(output, label)\n        \n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()*images.shape[0]\n        count += images.shape[0]\n    \n    return epoch_loss/count\n        \n\ndef valid_fn(valid_loader, model, criterion, device):\n    model.to(device).eval()\n    \n    loss = 0\n    count = 0\n    preds=[]\n    for i, data in enumerate(tqdm(valid_loader,total = len(valid_loader))):\n        images = data['image'].to(device)\n        label = data['label'].to(device)\n        \n        with torch.no_grad():\n            output = model(images)\n            step_loss = criterion(output, label)\n#             step_loss=FocalLoss(output,label)\n#             loss += step_loss.item()*images.shape[0]\n            loss+=step_loss*images.shape[0]\n            count += images.shape[0]\n#             output=nn.Softmax(dim=1)(output)\n#             output=torch.argmax(output,dim=1).to('cpu').numpy()\n#             preds.append(output)\n            preds.append(output.softmax(1).to('cpu').numpy())\n#             print(len(preds))\n#             preds.append(output.softmax(1).to('cpu').numpy())\n#     preds = np.concatenate(preds)\n#     preds = preds.argmax(1)    \n#             print(output.shape,label.shape)\n#             acc+=torch.sum(output==label)\n#    \n    preds = np.concatenate(preds)\n    preds = preds.argmax(1)\n    print(len(labels),len(preds))\n    f11=f1_score(labels,preds, average='macro')\n    return f11,loss/count\n    \n\ndef test_fn(test_loader, model, device):\n    model.to(device).eval()\n    \n    predictions = []\n    for i, data in enumerate(tqdm(test_loader,total = len(test_loader))):\n        images = data['image'].to(device)\n        \n        with torch.no_grad():\n            output = model(images)\n            predictions.append(output.softmax(1).to('cpu').numpy())\n            \n    predictions = np.concatenate(predictions)\n    predictions = predictions.argmax(1)\n    \n    return predictions   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the dataloaders and defining the loss function and optimizers","metadata":{}},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_loader = DataLoader(train_data, batch_size=CFG.batch_size, sampler=sampler, shuffle=False)\n# valid_loader = DataLoader(valid_data, batch_size=CFG.batch_size, shuffle=False)\ntrain_loader = DataLoader(dataset, batch_size=CFG.batch_size, sampler=sampler, shuffle=False)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CustomNet(model_name=CFG.model_name, pretrained=True)\n\nmodel.to(device)\nepochs = CFG.epochs\noptimizer = torch.optim.Adam(model.parameters(), lr=CFG.lr)\ncriterion = nn.CrossEntropyLoss()\n# criterion=torchvision.ops.sigmoid_focal_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Training Loop","metadata":{}},{"cell_type":"markdown","source":"Here we run the training loop and save the best possible model","metadata":{}},{"cell_type":"code","source":"best_val_loss = np.inf\nbest_acc=0\n# scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\nfor epoch in range(CFG.epochs):\n    train_loss = train_fn(train_loader, model, optimizer, criterion, device)\n# #     scheduler.step()\n#     acc,val_loss = valid_fn(valid_loader, model, criterion, device)\n\n#     if val_loss < best_val_loss and acc>best_acc:\n#         print('Loss Improved')\n#         best_val_loss = val_loss\n#         print('accuracy Improved')\n#         best_acc=acc\n#         print(f'Epoch {epoch+1} - Save Val_Loss: {best_val_loss:.4f}')\n#         print(f'Epoch {epoch+1} - Save accuracy: {best_acc:.4f}')\n#         torch.save({'model': model.state_dict(), \n#                     'optimizer': optimizer.state_dict()},\n#                     './'+f'{CFG.model_name}_best.pth')\n#     else :\n#         print(val_loss , acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Predictions","metadata":{}},{"cell_type":"markdown","source":"Since we have trained the model, its time to obtain the predictions on the data. It's preferable to first evaluate the validation data on an appropriate metric. After you are convinced with the results, proceed with the below cells to obtain the prediction on the test data.","metadata":{}},{"cell_type":"code","source":"test_dataset = TestData(transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\ncheck_point = torch.load('./'+f'{CFG.model_name}_best.pth')\nmodel = CustomNet(CFG.model_name, pretrained=True)\nmodel.to(device)\nmodel.load_state_dict(check_point['model'])\npred = test_fn(test_loader, model, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/ai-of-god-v20/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['Class'] = pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.Class.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('/kaggle/working/resnet18_oversampling.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Once you have reached this step, you can hit the submit button.\n# Congratulations on your submission! Your score will be displayed on the leaderboard after successful submission.","metadata":{}},{"cell_type":"markdown","source":"# This notebook is just a basic guide for the beginners. The participants are encouraged to create their own notebooks and approach the problem according to their own ideology and thinking.\n\n# All the best and have fun","metadata":{}},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}